
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/01-pref-hokkaido_dev.ipynb

import bs4
import re
import csv
import datetime
import requests
from collections import defaultdict

urls = ['http://www.pref.hokkaido.lg.jp/hf/kth/kak/hasseijoukyou.htm']

def trim_div(s):
    return re.sub('<div>|<div .+?>|</div>', '', s)

def get_src(url):
    response = requests.get(url)
    response.encoding = response.apparent_encoding
    s = trim_div(response.text)
    return bs4.BeautifulSoup(s, 'html.parser')

def get_links(urls):
    domain = 'http://www.pref.hokkaido.lg.jp'
    filter = '新型コロナウイルス.+患者.+発生'
    links = []
    for url in urls:
        src = get_src(url)
        links.extend(src.find_all(lambda tag: tag.name == 'a' and re.search(filter, tag.get_text()) != None))
    return [[domain + tag['href'], tag.get_text()] for tag in links]

def parse_links(links):
    refs = {}
    for link in links:
        nums = [re.sub("\\D", "", s) for s in re.split('[～、]', link[1])]
        if len(nums) == 1:
            refs[str(int(nums[0]))] = link[0]
        else:
            for i in range(int(nums[0]), int(nums[1])+ 1):
                refs[str(i)] = link[0]
    return refs

header = ['No','公表日','年代','性別','居住地','周囲の患者の発生','濃厚接触者の状況','url']

def parse(src, url, refs):

    patients = []
    tbody = src.body.find(lambda tag: tag.name == 'tbody')
    for tr in tbody.find_all('tr'):
        row = []
        for td in tr.find_all('td'):
            text = []
            for c in [c.string for c in td.contents if c.string]: # <br/> を削除
                if c != '\xa0' and c != '\n' and c != '\r\n' and len(c.strip()) != 0: # <br/> で区切られていたこれらの文字を削除
                    text.append(str(re.sub('[\xa0\r\n]', '', c))) # 文字列の一部として含まれる '\xa0' を削除
            row.append('\n'.join(text))
        if row[0].isdecimal() : row[0] = str(int(row[0])) # No を ascii へ変換
        row.append(refs.get(row[0], url)) # url を追加
        patients.append(row)
    return patients[1:]

def get_patient(url):
    links = get_links(url)
    refs = parse_links(links)
    src = get_src(url[0])
    return parse(src, url[0], refs)

def create_fname(base):
    now = datetime.datetime.now()
    return base + '_' + now.strftime('%Y%m%dT%H%M') + ".csv"

def write_csv(patients, fname):
    with open(fname, 'w') as f:
        writer = csv.writer(f)
        writer.writerows(patients)

def main():
    patients = sorted(get_patient(urls), key=lambda x: int(x[0]))
    patients.insert(0, header)
    fname = create_fname("data/01-pref-hokkaido")
    write_csv(patients, fname)
    print("created:", fname)

if __name__ == '__main__':
    main()