
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/11-city-kawagoe_dev.ipynb

import bs4
import re
import csv
import datetime
import requests
from collections import defaultdict

urls = ['https://www.city.kawagoe.saitama.jp/kenkofukushi/byoki_iryo/kansensho/COVID19-p-C.html']

def trim_div(s):
    return re.sub('<div>|<div .+?>|</div>', '', s)

def get_src(url):
    response = requests.get(url)
    response.encoding = response.apparent_encoding
    return bs4.BeautifulSoup(trim_div(response.text), 'html.parser')

def get_links(urls):
    domain = 'https://www.city.kawagoe.saitama.jp/'
    filter = '新型コロナウイルス.+患者.+発生'
    links = []
    for url in urls:
        src = get_src(url)
        links.extend(src.find_all(lambda tag: tag.name == 'a' and re.search(filter, tag.get_text()) != None))
    return [[domain + tag['href'], tag.get_text()] for tag in links]

header = ['No','年代','性別','職業','居住地','症状・経過','渡航歴・患者との接触歴','現在の症状','濃厚接触者','その他','url']

def parse(src, url):

    def get_text(tag):
        return tag.get_text().strip()

    def get_contents(src):
        h = src.body.find(lambda tag: tag.name == 'h3')
        return h.previous_sibling.next_siblings

    def parse_h3(e):
        no = re.sub("\\D", "", get_text(e))
        return no if len(no) > 0 else '0'

    def parse_ol(e):
        item = {}
        for li in e.find_all('li'):
            #print(type(li), li.name, li.get_text())
            if li.string:
                split = re.split('[：:]', get_text(li))
                if len(split) > 1:
                    item[split[0]] = split[1]
                else:
                    print(type(e), e.name, e.get_text())
            else:
                split = [c.string for c in li.contents if c.string] # <br/> を削除
                split[0] = re.sub('[：:]', '', split[0])
                item[split[0]] = split[1:]
        return item

    def dic2array(item):
        return [item[h] for h in header]

    patients = []
    item = defaultdict(lambda: '')
    for e in get_contents(src):
        if isinstance(e, bs4.element.Tag):
            if e.name == 'h2':
                break
            if e.name == 'h3':
                if len(item) > 0:
                    item[header[-1]] = url
                    patients.append(dic2array(item))
                    item = defaultdict(lambda: '')
                item[header[0]] = parse_h3(e)
            elif e.name == 'ol':
                item.update(parse_ol(e))
            else:
                print(type(e), e.name, e.get_text())
    if len(item) > 0:
        item['url'] = url
        patients.append(dic2array(item))
    return patients

def get_patient(url):
    src = get_src(url)
    return parse(src, url)

def get_patients(urls):
    patients = []
    for link in get_links(urls):
        print(link)
        patients.extend(get_patient(link[0]))
    return patients

def create_fname(base):
    now = datetime.datetime.now()
    return base + '_' + now.strftime('%Y%m%dT%H%M') + ".csv"

def write_csv(patients, fname):
    with open(fname, 'w') as f:
        writer = csv.writer(f)
        writer.writerows(patients)

def main():
    patients = sorted(get_patients(urls), key=lambda x: int(x[0]))
    patients.insert(0, header)
    fname = create_fname("data/11-city-kawagoe")
    write_csv(patients, fname)
    print("created:", fname)

if __name__ == '__main__':
    main()