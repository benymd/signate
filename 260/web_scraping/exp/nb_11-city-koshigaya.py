
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/11-city-koshigaya_dev.ipynb

import bs4
import re
import csv
import datetime
import requests
from collections import defaultdict

urls = ['https://www.city.koshigaya.saitama.jp/kurashi_shisei/fukushi/hokenjo/kansensho/index.html']

def trim_div(s):
    return re.sub('<div>|<div .+?>|</div>', '', s)

def get_src(url):
    response = requests.get(url)
    response.encoding = response.apparent_encoding
    s = trim_div(response.text)
    return bs4.BeautifulSoup(s, 'html.parser')

def get_links(urls):
    domain = 'https://www.city.koshigaya.saitama.jp'
    filter = '新型コロナウイルス.+患者.+発生'
    links = []
    for url in urls:
        src = get_src(url)
        links.extend(src.find_all(lambda tag: tag.name == 'a' and re.search(filter, tag.get_text()) != None))
    return [[domain + tag['href'], tag.get_text()] for tag in links]

header = ['No','年代','性別','国籍','職業','通勤','居住地','症状、経過','その他','url']

def parse(src, url):

    def get_text(tag):
        return tag.get_text().strip()

    def get_contents(src):
        h = src.body.find_all(lambda tag: tag.name == 'h2')
        for h in src.body.find_all(lambda tag: tag.name == 'h2'):
            t = h.get_text()
            if '〜' in t: continue # "nn例目〜nn例目"
            if '例目' in t: break
        return h.previous_sibling.next_siblings

    def parse_h2(e):
        no = re.sub("\\D", "", get_text(e))
        return no if len(no) > 0 else '0'

    def parse_table(e):
        table = []
        for tr in e.find_all('tr'):
            row = []
            for td in tr.find_all('td'):
                row.append(get_text(td))
            table.append(' '.join(row))
        return table

    def parse_ul(e):
        return [get_text(li) for li in e.find_all('li')]

    def parse_ol(e):
        item = {}
        for li in e.find_all('li'):
            split = re.split('[：：]', get_text(li))
            item[split[0]] = split[1]
        return item

    def parse_div(e):
        ul = []
        for c in e.descendants:
            if c.name == 'p':
                ul.append(get_text(c))
        return ul

    def dic2array(item):
        return [item[h] for h in header]

    patients = []
    item = defaultdict(lambda: '')
    for e in get_contents(src):
        if isinstance(e, bs4.element.Tag):
            if e.name == 'h2':
                if 'お問い合わせ' in e.get_text(): break
                if len(item) > 0:
                    item['url'] = url
                    patients.append(dic2array(item))
                    item = defaultdict(lambda: '')
                item[header[0]] = parse_h2(e)
            elif e.name == 'h3':
                pass
            elif e.name == 'table': # 症状、経過
                item[header[7]] = '\n'.join(parse_table(e)).strip()
            elif e.name == 'ul': # その他
                item[header[8]] = '\n'.join(parse_ul(e))
            elif e.name == 'ol':
                item.update(parse_ol(e))
            elif e.name == 'p':
                pass
            else:
                print(type(e), e.name)
    if len(item) > 0:
        item['url'] = url
        patients.append(dic2array(item))
    return patients

def get_patient(url):
    src = get_src(url)
    return parse(src, url)

def get_patients(urls):
    patients = []
    for link in get_links(urls):
        print(link)
        patients.extend(get_patient(link[0]))
    return patients

def create_fname(base):
    now = datetime.datetime.now()
    return base + '_' + now.strftime('%Y%m%dT%H%M') + ".csv"

def write_csv(patients, fname):
    with open(fname, 'w') as f:
        writer = csv.writer(f)
        writer.writerows(patients)

def main():
    patients = sorted(get_patients(urls), key=lambda x: int(x[0]))
    patients.insert(0, header)
    fname = create_fname("data/11-city-koshigaya")
    write_csv(patients, fname)
    print("created:", fname)

if __name__ == '__main__':
    main()